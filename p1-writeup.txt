a) Think of two different ways in which you could objectively evaluate the model’s output. Describe both evaluations in your text answer file. Pick one of the two methods and use it for objective evaluation in all subsequent problems.

When evaluating the model’s output, one way to do this is by using semantic similarity scoring. This method involves comparing the meaning of the model’s generated text with the reference endings provided in the dataset. Tools like SentenceTransformer let us encode the sentences into numerical embeddings, and then we measure how closely these embeddings align using cosine similarity. This is a good criteria because it doesn’t just look for exact word matches—it actually evaluates whether the two sentences mean the same thing, even if they’re worded differently. This makes it particularly useful for a task like this, where the model might generate creative, valid responses that aren’t identical to the reference text. However, it can be sensitive to nuances in how the embeddings are trained, which might lead to some less intuitive results.
Another option is to use the BLEU score, which is more straightforward. BLEU measures how much overlap there is between the model's output and the reference text by comparing sequences of words (n-grams). The more similar the sequences, the higher the BLEU score. This method is widely used in machine translation and is simple to compute using libraries like nltk or sacrebleu. While it’s a quick and easy way to measure how closely the output matches the ground truth, it has some limitations. BLEU tends to penalize creative responses that might be correct in meaning but use different words or phrasing than the reference, which could make it less ideal for this task.

b) Pick one such criterion (either from the examples, or a different one) to subjectively evaluate the model below. In the text file, say which criterion you picked and why.

For subjective evaluation, I would pick appropriateness of the ending given the beginning as the criterion. This involves judging how well the model-generated ending aligns with the story’s premise, initial sentence, and counterfactual. The reasoning here is that the task revolves around causal reasoning—how a small change in the initial story affects the outcome. If the ending fails to logically follow from the counterfactual or contradicts the premise, it would be considered less appropriate. This criterion is central to the dataset’s purpose and tests the model’s ability to reason and adapt its output in line with narrative changes.
Using this criterion, I would rate the outputs on a scale of 1 to 5, where 1 means that the ending is completely inappropriate or nonsensical given the context, and 5 means that the ending is fully appropriate, coherent, and aligns perfectly with the story context.